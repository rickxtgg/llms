{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# 在Kaggle上运行LLMs训练项目的详细步骤\n",
    "本notebook提供了在Kaggle环境中运行LLMs训练项目的完整、详细流程。每个步骤都包含具体的代码实现和必要的说明。\n",
    "## 1. 环境准备\n",
    "首先，我们需要克隆项目代码并安装所需的依赖项。所有依赖项都在`requirements.txt`中定义。"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["# 克隆项目代码\n",
    "!git clone https://github.com/your-username/llmscopy.git\n",
    "!cd llmscopy\n",
    "\n",
    "# 安装依赖项\n",
    "!pip install torch>=1.10.0 torchvision>=0.11.0 torchaudio>=0.10.0\n",
    "!pip install transformers>=4.15.0 datasets>=1.18.0 tokenizers>=0.10.3\n",
    "!pip install tensorboard>=2.7.0 wandb>=0.12.0 tqdm>=4.62.0\n",
    "!pip install scikit-learn>=1.0.0 numpy>=1.20.0 pandas>=1.3.0\n",
    "\n",
    "import os\n",
    "os.chdir('llmscopy')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 2. 数据准备\n",
    "### 2.1 数据格式说明\n",
    "训练数据应位于`data/processed/train.txt`，验证数据位于`data/processed/val.txt`。\n",
    "数据文件的格式要求：\n",
    "- 每行一个训练样本\n",
    "- 文本应该是预处理过的（清理过的）\n",
    "- 编码格式为UTF-8\n",
    "\n",
    "### 2.2 设置数据路径"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["# 设置数据路径\n",
    "TRAIN_DATA_PATH = '/kaggle/input/your-dataset/train.txt'\n",
    "VAL_DATA_PATH = '/kaggle/input/your-dataset/val.txt'\n",
    "\n",
    "# 检查数据文件\n",
    "!head -n 5 $TRAIN_DATA_PATH\n",
    "print(f'训练数据行数: {!wc -l $TRAIN_DATA_PATH}')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3. 配置模型和训练参数\n",
    "### 3.1 加载和修改配置\n",
    "我们使用`configs/model_config.py`中定义的配置，并根据Kaggle环境进行必要的调整。"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["from configs.model_config import get_small_config\n",
    "\n",
    "config = get_small_config()\n",
    "\n",
    "# 1. 路径配置\n",
    "config.train_file = TRAIN_DATA_PATH\n",
    "config.val_file = VAL_DATA_PATH\n",
    "config.output_dir = '/kaggle/working/checkpoints'\n",
    "\n",
    "# 2. 训练参数配置\n",
    "config.batch_size = 32  # 根据GPU显存调整\n",
    "config.gradient_accumulation_steps = 4  # 梯度累积步数\n",
    "config.max_steps = 50000  # 总训练步数\n",
    "config.warmup_steps = 2000  # 预热步数\n",
    "\n",
    "# 3. 优化器配置\n",
    "config.learning_rate = 5e-5\n",
    "config.weight_decay = 0.01\n",
    "config.adam_beta1 = 0.9\n",
    "config.adam_beta2 = 0.999\n",
    "config.adam_epsilon = 1e-8\n",
    "\n",
    "# 4. 混合精度训练配置\n",
    "config.fp16 = True  # 启用混合精度训练\n",
    "\n",
    "print('配置信息:\n', config)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 4. 训练流程\n",
    "### 4.1 数据加载和预处理\n",
    "使用`src/data/preprocessing.py`中的`TextDataset`类加载和处理数据。"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["from src.data.preprocessing import TextDataset\n",
    "from src.data.tokenizer import get_tokenizer\n",
    "\n",
    "# 1. 获取分词器\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "# 2. 创建训练集\n",
    "train_dataset = TextDataset(\n",
    "    file_path=config.train_file,\n",
    "    tokenizer=tokenizer,\n",
    "    block_size=config.n_positions,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# 3. 创建验证集\n",
    "val_dataset = TextDataset(\n",
    "    file_path=config.val_file,\n",
    "    tokenizer=tokenizer,\n",
    "    block_size=config.n_positions,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f'训练集大小: {len(train_dataset)}')\n",
    "print(f'验证集大小: {len(val_dataset)}')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 4.2 初始化模型和训练器\n",
    "使用`src/models/transformer.py`中的`GPT`模型和`src/training/trainer.py`中的`Trainer`类。"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["from src.models.transformer import GPT\n",
    "from src.training.trainer import Trainer\n",
    "\n",
    "# 1. 初始化模型\n",
    "model = GPT(config)\n",
    "\n",
    "# 2. 打印模型信息\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'模型总参数量: {total_params:,}')\n",
    "\n",
    "# 3. 初始化训练器\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset\n",
    ")"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 4.3 开始训练\n",
    "训练过程将自动执行以下操作：\n",
    "1. 每10步记录训练损失\n",
    "2. 每1000步进行验证\n",
    "3. 每5000步保存检查点\n",
    "4. 保存验证损失最低的模型"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["# 开始训练\n",
    "global_step, best_val_loss = trainer.train()\n",
    "\n",
    "print(f'训练完成！')\n",
    "print(f'总训练步数: {global_step}')\n",
    "print(f'最佳验证损失: {best_val_loss:.4f}')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 5. 保存和导出结果\n",
    "训练结果将保存在以下位置：\n",
    "- 模型检查点：`/kaggle/working/checkpoints/checkpoint-{step}/`\n",
    "- 最佳模型：`/kaggle/working/checkpoints/best_model/`\n",
    "- 最终模型：`/kaggle/working/checkpoints/final_model/`\n",
    "- TensorBoard日志：`/kaggle/working/runs/`\n",
    "\n",
    "### 重要提示\n",
    "1. 确保在Kaggle notebook设置中启用GPU\n",
    "2. 定期下载重要的checkpoint，因为Kaggle notebook重启后数据会丢失\n",
    "3. 可以使用TensorBoard查看训练过程中的损失曲线和学习率变化"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}